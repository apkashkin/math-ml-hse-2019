{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Машинное обучение\n",
    "\n",
    "## Факультет математики НИУ ВШЭ\n",
    "\n",
    "### 2019-2020 учебный год\n",
    "\n",
    "Илья Щуров, Соня Дымченко, Руслан Хайдуров, Александр Каган, Павел Балтабаев\n",
    "\n",
    "# Семинар 15 (16)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means\n",
    "\n",
    "<img src=\"http://dendroid.sk/wp-content/uploads/2013/01/kmeansimg-scaled1000.jpg\" width=400>\n",
    "\n",
    "Алгоритм k-means применяется для задачи кластеризации. Напомню его основные шаги. Пусть у нас имеется $N$ точек на плоскости ${(x_1,y_1),...,(x_N,y_N)}$\n",
    "Допустим мы хотим разбить их на $k=2$ кластера.\n",
    "1. Выбираем $k=2$ случайные точки из этого множества. Говорим, что они являются теперь центрами наших кластеров.\n",
    "2. Для каждой из оставшихся точек смотрим, к какому из центров она ближе и определяем её в этот кластер.\n",
    "3. У нас получилось разбить точки на 3 кластера. Естественно это не оптимальное разбиение. Найдём новые центры кластеров. Например, если точки ${(x_{i_1}, y_{i_1}),...,(x_{i_n}, y_{i_n})}$ попали в один кластер, то их новый центр будет имеет координаты:\n",
    "$$x_M=\\frac{x_{i_1}+...+x_{i_n}}{n}$$\n",
    "$$y_M=\\frac{y_{i_1}+...+y_{i_n}}{n}$$\n",
    "4. Переходим к шагу 2 и продолжаем до тех пор, пока кластеры не перестанут меняться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ссылка на визуализацию](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs, make_circles\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "X_noised, y_noised = make_blobs(n_samples=1500, centers=3, cluster_std=2.1, random_state=42)\n",
    "plt.subplot(131)\n",
    "plt.scatter(X_noised[:, 0], X_noised[:, 1], edgecolors='face')\n",
    "plt.axis('equal') # чтобы рисунок был не растянут\n",
    "plt.title(u\"Круглые кластеры\", fontsize=15)\n",
    "\n",
    "transformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]] # матрица преобразования\n",
    "X_long = np.dot(X_noised, transformation) # умножим на нее, чтобы растянуть кучки\n",
    "y_long = y_noised\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.scatter(X_long[:, 0], X_long[:, 1], edgecolors='face')\n",
    "plt.title(u\"Вытянутые кластеры\", fontsize=15)\n",
    "\n",
    "X_circles, y_circles = make_circles(n_samples=1500, factor=0.5, noise=0.05)\n",
    "plt.subplot(133)\n",
    "plt.scatter(X_circles[:, 0], X_circles[:, 1], edgecolors='face')\n",
    "plt.title(u\"Кластеры сложной структуры\", fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging = dict(blob=[], long=[], circle=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means из sklearn\n",
    "\n",
    "Воспользуйтесь реализацией алгоритма из библиотеки sklearn и провизуализируйте результат кластеризации на $k=2,3,5,7$ классов для трех датасетов выше, сохраните результат, провизуализируйте (получится сетка графиков 4 на 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# твой код тут # (つ▀¯▀)つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Спектральная кластеризация\n",
    "\n",
    "Состоит из 3х шагов:\n",
    "\n",
    "1. Для входных данных размерностью $(N, d)$ считаем матрицу схожести $A$ размерностью $(N,N)$. В простейшем случае это могут быть попарные расстояния, но на самом деле матрица может быть получена и с использованием более сложных операций над парами точек \n",
    "2. Полученная матрица симметрична. Вычисляем для нее матрицу Лапласиана:\n",
    "    $${\\displaystyle L=D-A}$$\n",
    "    где ${\\displaystyle D}$  диагональная матрица, такая что:\n",
    "    $${\\displaystyle D_{ii}=\\sum _{j}A_{ij}.}$$\n",
    "    В самом алгоритме используется нормализованная версия Лапласиана\n",
    "3. Подсчет собственных векторов для матрицы Лапласиана и кластеризация этих векторов подручными методами, например, с помощью того же K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "\n",
    "def laplacian(A):\n",
    "    \"\"\"Computes the symetric normalized laplacian.\n",
    "    L = D^{-1/2} A D^{-1/2}\n",
    "    \"\"\"\n",
    "    D = np.zeros(A.shape)\n",
    "    w = np.sum(A, axis=0)\n",
    "    D.flat[::len(w) + 1] = w ** (-0.5)  # set the diag of D to w\n",
    "    return D.dot(A).dot(D)\n",
    "\n",
    "\n",
    "def k_means(X, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=1231)\n",
    "    return kmeans.fit(X).labels_\n",
    "\n",
    "\n",
    "def spectral_clustering(affinity, n_clusters, cluster_method=k_means):\n",
    "    L = laplacian(affinity)\n",
    "    eig_val, eig_vect = sp.sparse.linalg.eigs(L, n_clusters)\n",
    "    X = eig_vect.real\n",
    "    rows_norm = np.linalg.norm(X, axis=1, ord=2)\n",
    "    Y = (X.T / rows_norm).T\n",
    "    labels = cluster_method(Y, n_clusters)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pairwise_distances(X_noised)\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = spectral_clustering(A, n_clusters=3)\n",
    "points_x = X_noised[:,0]\n",
    "points_y = X_noised[:,1]\n",
    "plt.title('Спектральная кластеризация')\n",
    "plt.scatter(x=points_x, y=points_y, c=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Спектральная кластеризация из sklearn\n",
    "\n",
    "Сделайте кластеризацию с помощью sklearn для тех же трех датасетов с числом кластеров $k=3,4,5,7$, сохраните результат, провизуализируйте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import spectral_clustering\n",
    "\n",
    "# твой код тут # (つ▀¯▀)つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Агломеративная кластеризация\n",
    "\n",
    "Наверное самый простой и понятный алгоритм кластеризации без фиксированного числа кластеров — агломеративная кластеризация.\n",
    "\n",
    "Интуиция у алгоритма очень простая:\n",
    "\n",
    "1. Начинаем с того, что высыпаем на каждую точку свой кластер\n",
    "2. Сортируем попарные расстояния между центрами кластеров по возрастанию\n",
    "3. Берём пару ближайших кластеров, склеиваем их в один и пересчитываем центр кластера\n",
    "4. Повторяем п. 2 и 3 до тех пор, пока все данные не склеятся в один кластер\n",
    "\n",
    "Сам процесс поиска ближайших кластеров может происходить с использованием разных методов объединения точек:\n",
    "\n",
    "* Single linkage — минимум попарных расстояний между точками из двух кластеров\n",
    "* Complete linkage — максимум попарных расстояний между точками из двух кластеров\n",
    "* Average linkage — среднее попарных расстояний между точками из двух кластеров\n",
    "* Centroid linkage — расстояние между центроидами двух кластеров\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "X = np.zeros((150, 2))\n",
    "\n",
    "np.random.seed(seed=42)\n",
    "X[:50, 0] = np.random.normal(loc=0.0, scale=.3, size=50)\n",
    "X[:50, 1] = np.random.normal(loc=0.0, scale=.3, size=50)\n",
    "\n",
    "X[50:100, 0] = np.random.normal(loc=2.0, scale=.5, size=50)\n",
    "X[50:100, 1] = np.random.normal(loc=-1.0, scale=.2, size=50)\n",
    "\n",
    "X[100:150, 0] = np.random.normal(loc=-1.0, scale=.2, size=50)\n",
    "X[100:150, 1] = np.random.normal(loc=2.0, scale=.5, size=50)\n",
    "\n",
    "distance_mat = pdist(X) # pdist посчитает нам верхний треугольник матрицы попарных расстояний\n",
    "\n",
    "Z = hierarchy.linkage(distance_mat, 'single') # linkage — реализация агломеративного алгоритма\n",
    "plt.figure(figsize=(18, 5))\n",
    "dn = hierarchy.dendrogram(Z, color_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аггломеративная кластеризация из sklearn\n",
    "С помощью sklearn сделаем аггломеративную кластеризацию с числом кластеров $k=3,10$ для разных параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "data = X_noised # X_circles, X_long\n",
    "\n",
    "knn_graph = kneighbors_graph(data, 10, include_self=False)\n",
    "\n",
    "for connectivity in (None, knn_graph):\n",
    "    for n_clusters in (3, 10):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        for index, linkage in enumerate(('average',\n",
    "                                         'complete',\n",
    "                                         'ward')):\n",
    "            plt.subplot(1, 3, index + 1)\n",
    "            model = AgglomerativeClustering(linkage=linkage,\n",
    "                                            connectivity=connectivity,\n",
    "                                            n_clusters=n_clusters)\n",
    "            t0 = time.time()\n",
    "            model.fit(data)\n",
    "            elapsed_time = time.time() - t0\n",
    "            plt.scatter(data[:, 0], data[:, 1], c=model.labels_,\n",
    "                        cmap=plt.cm.nipy_spectral)\n",
    "            plt.title('linkage=%s\\n(time %.2fs)' % (linkage, elapsed_time),\n",
    "                      fontdict=dict(verticalalignment='top'))\n",
    "            plt.axis('equal')\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.subplots_adjust(bottom=0, top=0.8, wspace=0,\n",
    "                                left=0, right=1)\n",
    "            plt.suptitle('n_cluster=%i, connectivity=%r' %\n",
    "                         (n_clusters, connectivity is not None), size=17)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метрики кластеризации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted Rand Index (ARI)\n",
    "\n",
    "Как можно догадаться по названию, это \"подкрученная\" метрика `Random Index`:\n",
    "\n",
    "${\\displaystyle RI(S)={\\frac {a+b}{a+b+c+d}}={\\frac {a+b}{n \\choose 2}}}$\n",
    "\n",
    "Для набора $S$ из $n$ элементов и его двух возможных разбиений на кластера  $X$ и $Y$\n",
    "\n",
    "* $  a$, количество пар элементов в $  S$, которые находятся в одном и том же подмножестве в  $X$ и в одном и том же подмножестве в $  Y$\n",
    "* $b$, количество пар элементов в $S$, которые находятся в разных подмножествах в $ X$ и в разных подмножествах в $  Y$\n",
    "\n",
    "* $  c$, количество пар элементов в $ S$, которые находятся в одном и том же подмножестве в $  X$ и в разных подмножествах в $  Y$\n",
    "* $  d$, количество пар элементов в $ S$, которые находятся в разных подмножествах в $  X$ и в одном и том же подмножестве в $  Y$\n",
    "\n",
    " Метрика $ARI$ делает поправку на то, чтобы случайное разметка $S$ на кластера имела бы метрику равной нулю.\n",
    "\n",
    "$${\\displaystyle \\overbrace {ARI} ^{\\text{Adjusted Index}}={\\frac {\\overbrace {\\sum _{ij}{\\binom {n_{ij}}{2}}} ^{\\text{Index}}-\\overbrace {[\\sum _{i}{\\binom {a_{i}}{2}}\\sum _{j}{\\binom {b_{j}}{2}}]/{\\binom {n}{2}}} ^{\\text{Expected Index}}}{\\underbrace {{\\frac {1}{2}}[\\sum _{i}{\\binom {a_{i}}{2}}+\\sum _{j}{\\binom {b_{j}}{2}}]} _{\\text{Max Index}}-\\underbrace {[\\sum _{i}{\\binom {a_{i}}{2}}\\sum _{j}{\\binom {b_{j}}{2}}]/{\\binom {n}{2}}} _{\\text{Expected Index}}}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silhouette\n",
    "\n",
    "Если вдруг все же надо подобрать значение k достаточно хорошим, можно смотреть на визуализацию метрики силуэт.\n",
    "\n",
    "Для ее подсчета не требуется знать истинную разметку, она лишь характеризует текущую.\n",
    "\n",
    "Wiki:\n",
    ">Let ${\\displaystyle b(i)} $ be the smallest average distance of ${\\displaystyle i}$  to all points in any other cluster, of which ${\\displaystyle i}$  is not a member. The cluster with this smallest average dissimilarity is said to be the \"neighbouring cluster\" of ${\\displaystyle i}$  because it is the next best fit cluster for point ${\\displaystyle i}$. We now define a silhouette:\n",
    "\n",
    "$${\\displaystyle s(i)={\\frac {b(i)-a(i)}{\\max\\{a(i),b(i)\\}}}}$$\n",
    "\n",
    "Таким образом, чем выше ситуэт для объекта, тем глубже этот объект находится в своем кластере.\n",
    "\n",
    "Это также может помочь детектировать выбросы в данных.\n",
    "\n",
    "Пример визуализации:\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/Silhouette-plot-orange.png/800px-Silhouette-plot-orange.png\" width=400>\n",
    "\n",
    "[Пример подбора параметра K для Kmeans с помощью `Silhouette`](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посчитаем метрики\n",
    "Посчитайте метрику ARI для всех использованных вами алгоритмов кластеризации для датасетов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# твой код тут # (つ▀¯▀)つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кластеризация картинки\n",
    "\n",
    "Найдите в интернете картинку, подгрузите ее и примените алгоритм кластеризации. \n",
    "\n",
    "Посмотрите что вышло."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import urllib\n",
    "\n",
    "def load_image(infilename):\n",
    "    if infilename.startswith('https'):\n",
    "        f = urllib.request.urlopen(infilename)\n",
    "        img = Image.open(f)\n",
    "    else:\n",
    "        img = Image.open(infilename)\n",
    "    img.load()\n",
    "    data = np.asarray(img, dtype=\"float32\")\n",
    "    return data\n",
    "\n",
    "def save_image(img,outfilename):\n",
    "    img.save(outfilename)\n",
    "    \n",
    "\n",
    "url = # твоя ссылка тут # (つ▀¯▀)つ\n",
    "pic = load_image(url)\n",
    "print(\"image shape: \", pic.shape)\n",
    "plt.imshow(pic[:,:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для картинки нужно уменьшить число цветов в палитре; для этого нужно выделить кластеры в пространстве RGB, объекты соответствуют пикселям изображения; после выделения кластеров, все пиксели, отнесенные в один кластер, заполняются одним цветом; этот цвет может быть центроидом соответствующего кластера, медианным цветом по кластеру. Попробуйте различные алгоритмы кластеризации:\n",
    "\n",
    "* KMeans\n",
    "* DBSCAN\n",
    "Можно использовать и другие:\n",
    "\n",
    "* MeanShift\n",
    "* AgglomerativeClustering\n",
    "\n",
    "Какие угодно, какие сможете найти\n",
    "Рассмотрите число кластеров K = 2, 3, 10, 20 (в алгоритмах, где есть такой гиперпараметр).\n",
    "\n",
    "Для различных кластеризаций оцените и сравните потери от уменьшения цветов при помощи метрики [SSIM](http://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.compare_ssim). Какой способ оказался лучшим?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
